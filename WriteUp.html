<h2>
    Background
</h2>
<p>
    Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively
    inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to
    improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a
    particular activity they do, but they rarely quantify how well they do it.
</p>
<p>
    In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts
    correctly and incorrectly in 5 different ways.
</p>
<p>
    More information is available from the website here: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> (see the
    section on the Weight Lifting Exercise Dataset).
</p>
<p>
    The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.
</p>
<h3>
    <a id="user-content-data" href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#data">
        <svg height="16" width="16">
        </svg>
    </a>
    Data
</h3>
<p>
    The training data for this project are available here:
</p>
<p>
    <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a>
</p>
<p>
    The test data are available here:
</p>
<p>
    <a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a>
</p>
<p>
    The data for this project come from this source: <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>.
</p>
<h3>
    <a id="user-content-data-preparation" href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#data-preparation">
        <svg height="16" width="16">
        </svg>
    </a>
    Data Preparation
</h3>
<p>
    Load the training and testing data sets, changing the values "#DIV/0!" and "" to NA.
</p>
<div>
    <pre>&gt; train&lt;-read.csv("~/Desktop/PracticalMachineLearning/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
&gt; test&lt;-read.csv("~/Desktop/PracticalMachineLearning/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))</pre>
</div>
<p>
    Check the dimensions of the training and testing data sets
</p>
<div>
    <pre>&gt; dim(train)
[1] 19622   160

&gt; dim(test)
[1]  20 160</pre>
</div>
<p>
    By looking at the column names, we see that some of them, namely the first seven ("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2",
    "cvtd_timestamp", "new_window", "num_window") are irrelevant for building a model, so we delete them.
</p>
<div>
    <pre>&gt; train&lt;-train[,-c(1:7)]
&gt; test&lt;-test[,-c(1:7)]</pre>
</div>
<p>
    Now, first delete the columns with all missing values.
</p>
<div>
    <pre>&gt; train&lt;-train[, colSums(is.na(train)) != nrow(train)]
&gt; test&lt;-test[, colSums(is.na(test)) != nrow(test)]
&gt; dim(train)
[1] 19622   147
&gt; dim(test)
[1] 20 53</pre>
</div>
<p>
    If we delete all the variables with missing values, then the dimensions of the training and testing sets will match:
</p>
<div>
    <pre>&gt; dim(train[,colSums(is.na(train)) == 0])
[1] 19622    53
&gt; dim(test[,colSums(is.na(test)) == 0])
[1] 20 53</pre>
</div>
<p>
    Notice that one of the variable names doesn't macth:
</p>
<div>
    <pre>&gt; colnames(test[,colSums(is.na(test)) == 0])==colnames(train[,colSums(is.na(train)) == 0])

 [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[14]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[27]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[40]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[53] FALSE</pre>
</div>
<p>
    By checking the variable names we see that it is variable "classe" (response) for the training set, and "problem_id" for the testing set. So, we delete the
    "problem_id" variable and fix our training and testing data sets.
</p>
<div>
    <pre>&gt; train&lt;-train[,colSums(is.na(train)) == 0]
&gt; test&lt;-test[,colSums(is.na(test)) == 0]
&gt; test&lt;-test[,-c(53)]</pre>
</div>
<p>
    Test set doesn't have the "classe" variable, since that is the one we are going to predict. So, our model will have 52 predictors.
</p>
<h3>
    <a id="user-content-cross-validation" href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#cross-validation">
        <svg height="16" width="16">
        </svg>
    </a>
    Cross Validation
</h3>
<p>
    The test set is large enough (19622 observations) to be devided into sub-training set (on which we will build the model) and sub-testing set (on which we
    will test the model and estimate the out-of-sample error)
</p>
<div>
    <pre>&gt;&gt; inTrain&lt;-createDataPartition(y=train$classe,p=0.75,list=FALSE)
&gt; subtrain&lt;-train[inTrain,]
&gt; subtest&lt;-train[-inTrain,]
&gt; dim(subtrain)
[1] 14718    53
&gt; dim(subtest)
[1] 4904   53
&gt; dim(test)
[1] 20 52</pre>
</div>
<h2>
    <a id="user-content-building-the-models" href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#building-the-models">
        <svg height="16" width="16">
        </svg>
    </a>
    Building the models
</h2>
<h5>
    <a
        id="user-content-first-prediction-model-using-decision-tree"
        href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#first-prediction-model-using-decision-tree"
    >
        <svg height="16" width="16">
        </svg>
    </a>
    First prediction model: Using Decision Tree
</h5>
<div>
    <pre>&gt; library(rpart)
&gt; library(rpart.plot)
&gt; model1 &lt;- rpart(classe ~ ., data=subtrain, method="class")
&gt; prediction1 &lt;- predict(model1, subtest, type = "class")
&gt;rpart.plot(model1, extra=102, under=TRUE, faclen=0)</pre>
</div>
<p>
    Test results on the subtest data set
</p>
<div>
    <pre>&gt; confusionMatrix(prediction1, subtest$classe)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1242  171   24   63   29
         B   49  569   82   59   72
         C   28   86  677  114   99
         D   56   80   52  524   66
         E   20   43   20   44  635

Overall Statistics

               Accuracy : 0.7437          
                 95% CI : (0.7312, 0.7559)
    No Information Rate : 0.2845          
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

                  Kappa : 0.6748          
 Mcnemar's Test P-Value : &lt; 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.8903   0.5996   0.7918   0.6517   0.7048
Specificity            0.9182   0.9338   0.9192   0.9380   0.9683
Pos Pred Value         0.8123   0.6847   0.6743   0.6735   0.8333
Neg Pred Value         0.9547   0.9067   0.9544   0.9321   0.9358
Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
Detection Rate         0.2533   0.1160   0.1381   0.1069   0.1295
Detection Prevalence   0.3118   0.1695   0.2047   0.1586   0.1554
Balanced Accuracy      0.9043   0.7667   0.8555   0.7949   0.8365
&gt;</pre>
</div>
<h5>
    <a
        id="user-content-second-prediction-model-using-random-forest"
        href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#second-prediction-model-using-random-forest"
    >
        <svg height="16" width="16">
        </svg>
    </a>
    Second prediction model: Using Random Forest
</h5>
<div>
    <pre>&gt; library(randomForest)
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.

&gt; model2 &lt;- randomForest(classe ~. , data=subtrain, method="class")</pre>
</div>
<p>
    Test results on the subtest data set
</p>
<div>
    <pre>&gt; prediction2 &lt;- predict(model2, subtest, type = "class")
&gt; confusionMatrix(prediction2, subtest$classe)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1395    4    0    0    0
         B    0  943    2    0    0
         C    0    2  852    4    0
         D    0    0    1  800    6
         E    0    0    0    0  895

Overall Statistics

               Accuracy : 0.9961         
                 95% CI : (0.994, 0.9977)
    No Information Rate : 0.2845         
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      

                  Kappa : 0.9951         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            1.0000   0.9937   0.9965   0.9950   0.9933
Specificity            0.9989   0.9995   0.9985   0.9983   1.0000
Pos Pred Value         0.9971   0.9979   0.9930   0.9913   1.0000
Neg Pred Value         1.0000   0.9985   0.9993   0.9990   0.9985
Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837
Detection Rate         0.2845   0.1923   0.1737   0.1631   0.1825
Detection Prevalence   0.2853   0.1927   0.1750   0.1646   0.1825
Balanced Accuracy      0.9994   0.9966   0.9975   0.9967   0.9967
&gt;</pre>
</div>
<h3>
    <a id="user-content-conclusion" href="https://github.com/mterzi/MachineLearningProject/blob/master/WriteUp.md#conclusion">
        <svg height="16" width="16">
        </svg>
    </a>
    Conclusion
</h3>
<p>
    We choose the Random Forest model since its accuracy is very high, 0.9961 (vs. 0.7437). The expected out-of-sample error is 1 - accuracy = 0.0039, or
    0.39%, so, we can expect that very few, or none, of the 20 test samples will be missclassified.
</p>
